# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vKiZ_lagmy2j61-L1umI3x748YfENweg
"""

##EDA on resistance data
##By: Mustafain Ali, Tinghao Huang, Dominique Hughes, Chiara Mattamira, Haejun (Stella) Oh

import numpy as np
import os
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from matplotlib.ticker import MaxNLocator

##originally working in google colab
##change folder_path to appropriate path
##ex: folder_path = '.../fall-2025-antibiotic-effectiveness/Data/microbiology_cultures_cohort_4_11_25_version.csv'

from google.colab import drive
drive.mount('/content/drive')
folder_path = '/content/drive/MyDrive/DS_Project_2025/ARMD_data/CSV_files/'

df = pd.read_csv(f'{folder_path}microbiology_cultures_cohort_4_11_25_version.csv')

df.head(10)

##data cleaning

#create year column
df['order_time_jittered_utc'] = pd.to_datetime(df['order_time_jittered_utc'])
df['year'] = df['order_time_jittered_utc'].dt.year

#only want data when there was an organism
df = df[df['was_positive']==1]

#don't want any Null data
df = df[df['organism'] != 'Null']
df = df[df['susceptibility'] != 'Null']

# # of unique patients, bacteria, and antibiotics
print("\nUnique patients:", df['anon_id'].nunique())
print("Unique organisms:", df['organism'].nunique())
print("Unique antibiotics:", df['antibiotic'].nunique())

# check that there is no missing data
missing = df.isnull().mean().sort_values(ascending=False)
print("\n% Missing values (top 10):")
print((missing * 100).head(10))

# Overall S vs R distribution
print("\nResistance distribution:")
print(df['susceptibility'].value_counts(normalize=True))

#Top organisms
n = 5
print(f"\nTop {n} organisms:")
print(df['organism'].value_counts().head(n))

##information for saving plots
plotdir = '/content/drive/MyDrive/DS_Project_2025/EDA_Preprocessing/'

# Count number of samples per antibiotic per year
import matplotlib.pyplot as plt

# Count number of samples per organism per year
org_yearly_counts = (
    df.groupby(["year", "organism"])
      .size()
      .reset_index(name="count")
)

# Focus on top 5 most common organisms overall
top_orgs = df["organism"].value_counts().head(5).index
org_yearly_counts_top = org_yearly_counts[org_yearly_counts["organism"].isin(top_orgs)]

fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(1,1,1)
for org in top_orgs:
    subset = org_yearly_counts_top[org_yearly_counts_top["organism"] == org]
    ax.plot(subset["year"], subset["count"], marker="o", label=org)

ax.set_title("Number of Samples Tested per Year (Top 5 Organisms)")
ax.set_xlabel("Year")
ax.set_ylabel("Count of Tests")
ax.legend(title="Organism", bbox_to_anchor=(1.05, 1), loc="upper left")
ax.grid(True)
plt.tight_layout()
plt.show()
fig.savefig(os.path.join(plotdir, 'number_of_samples_tested_per_year_top_5_organisms.png'), dpi=300)

#looks like 2007 and 2024 have very low data amounts - so we remove them
df = df[df['year'] != 2007]
df = df[df['year'] != 2024]

##let's look at resistance trends over the years
#only want to look at antibiotics that were tested every year
#will look over all possible organisms

#create resistant column
df['resistant'] = (df['susceptibility']=='Resistant').astype(int)
years = df['year'].unique()

##create plotting information
antibiotics = df['antibiotic'].unique()
antibiotic_by_year = {}
antibiotic_by_month = {}

for i, anti in enumerate(antibiotics):
  df_anti = df[df.antibiotic == anti]
  years_for_anti = len(df_anti['year'].unique())
  if years_for_anti == len(years):
    year_data = df_anti.groupby('year').agg(
          subs = ('anon_id','count'),
          unique_subs = ('anon_id','nunique'),
          unique_bacteria = ('organism','nunique'),
          resistant = ('resistant','sum'),
          total = ('susceptibility','count')
    ).reset_index()
    antibiotic_by_year[anti] = year_data

##plot resistance per year for each antibiotic
full_antibiotics = antibiotic_by_year.keys()
n_antibiotics = len(full_antibiotics)

# Figure out subplot grid (e.g. 3 columns)
ncols = 3
nrows = -(-n_antibiotics // ncols)  # ceiling division

fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5*nrows), sharex=False, sharey=True)

# Flatten axes for easy looping
axes = axes.flatten()

for ax, name in zip(axes, full_antibiotics):
    df_working = antibiotic_by_year[name]
    ax.plot(df_working.year, df_working.resistant/df_working.total, marker='o')
    # ax.legend(loc = 'upper right')
    ax.set_title(name)
    ax.xaxis.set_major_locator(MaxNLocator(integer=True))

# Remove unused subplots if any
for i in range(len(full_antibiotics), len(axes)):
    fig.delaxes(axes[i])

fig.suptitle('%AMR Time Trends for Each Antibiotic By Year', fontsize=16)
fig.supxlabel('Year')
fig.supylabel('%AMR')

plt.tight_layout(rect=[0, 0.01, 1, 0.97])
plt.show()
fig.savefig(os.path.join(plotdir, 'percent_AMR_time_trends_for_each_antibiotic_by_year.png'), dpi=300)

#weird that some antibiotics seem to have a drop in resistance around 2016?
#let's look at rate of testing for antibiotics

ab_yearly = (
    df.groupby(["year", "antibiotic"])
      .size()
      .reset_index(name="count")
)

# Create pivoted table of counts
pivot = ab_yearly.pivot(index="year", columns="antibiotic", values="count").fillna(0)

# Normalize each year to percent
pivot_pct = pivot.div(pivot.sum(axis=1), axis=0) * 100

# Focus on top 6 antibiotics for readability
top_abs = df["antibiotic"].value_counts().head(15).index
ax = pivot_pct[top_abs].plot.area(figsize=(10,6), cmap="tab20")

ax.set_title("Share of Antibiotic Tests Over Time (Top 15 Antibiotics)")
ax.set_xlabel("Year")
ax.set_ylabel("% of Total Tests")
ax.legend(title="Antibiotic", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

fig = ax.get_figure()
fig.savefig(os.path.join(plotdir, 'share_of_antibiotic_tests_over_time_top_15_antibiotics.png'), dpi=300)

##we can see the amount tested for some antibiotics tends to drop slightly at 2016/2017
##same antibiotic amounts are raised at the same year
#research revealed that this is an old vs new antibiotics issue
#we can see this through plotting

## Year of First Commercial / Regulatory Use: 1950-1980
old_abs = [
 "Gentamicin", "Cefazolin", "Tetracycline",
    "Trimethoprim/Sulfamethoxazole", "Ampicillin", "Nitrofurantoin",  "Amikacin"
]


## Year of First Commercial / Regulatory Use: 1980 onward (newest one is 2001)
new_abs = ["Levofloxacin",   "Ciprofloxacin",
    "Ceftriaxone", "Piperacillin/Tazobactam", "Ceftazidime", "Meropenem", "Ertapenem",
    "Amoxicillin/Clavulanic Acid"
]


# Compute yearly mean resistance for each antibiotic
ab_yearly_res = (
    df.groupby(["year", "antibiotic"])["resistant"]
      .mean()
      .reset_index()
)

# Split into old vs new
old = (
    ab_yearly_res[ab_yearly_res["antibiotic"].isin(old_abs)]
    .groupby("year")["resistant"]
    .mean()
    .reset_index(name="old_resistance")
)

new = (
    ab_yearly_res[ab_yearly_res["antibiotic"].isin(new_abs)]
    .groupby("year")["resistant"]
    .mean()
    .reset_index(name="new_resistance")
)

# Merge for plotting
merged = pd.merge(old, new, on="year", how="outer").sort_values("year")

# Compute yearly counts per antibiotic ---
ab_yearly_counts = (
    df.groupby(["year", "antibiotic"])
      .size()
      .reset_index(name="count")
)


# Compute yearly counts for old vs new ---
old_counts = (
    ab_yearly_counts[ab_yearly_counts["antibiotic"].isin(old_abs)]
    .groupby("year")["count"]
    .sum()
    .rename("old_count")
)
new_counts = (
    ab_yearly_counts[ab_yearly_counts["antibiotic"].isin(new_abs)]
    .groupby("year")["count"]
    .sum()
    .rename("new_count")
)

counts = pd.concat([old_counts, new_counts], axis=1).fillna(0)
counts["total"] = counts["old_count"] + counts["new_count"]
counts["old_share"] = counts["old_count"] / counts["total"] * 100
counts["new_share"] = counts["new_count"] / counts["total"] * 100

# Compute yearly resistance (already done above, but recompute here for clarity) ---
ab_yearly_res = (
    df.groupby(["year", "antibiotic"])["resistant"]
      .mean()
      .reset_index()
)

old_res = (
    ab_yearly_res[ab_yearly_res["antibiotic"].isin(old_abs)]
    .groupby("year")["resistant"]
    .mean()
    .rename("old_res")
)
new_res = (
    ab_yearly_res[ab_yearly_res["antibiotic"].isin(new_abs)]
    .groupby("year")["resistant"]
    .mean()
    .rename("new_res")
)

res_rates = pd.concat([old_res, new_res], axis=1)

# Merge both for shared x-axis ---
merged = counts.merge(res_rates, left_index=True, right_index=True, how="outer").sort_index()


plt.plot(merged.index, merged["old_share"], marker="o", label="Old Antibiotics", color="red")
plt.plot(merged.index, merged["new_share"], marker="o", label="New Antibiotics", color="blue")
plt.ylabel("Share of Total Tests (%)")
plt.legend(loc="upper right")
plt.grid(True, linestyle="--", alpha=0.5)
plt.title('Old vs New Antibiotics, Share of Total Tests per Year')
plt.savefig(os.path.join(plotdir, 'old_vs_new_antibiotics_share_of_total_tests_per_year.png'), dpi=300)

##lets get a sense of the imbalance from our data
# Calculate yearly totals and resistant counts
yearly_counts = df['year'].value_counts().sort_index()
yearly_resistant = df.groupby('year')['resistant'].sum()
yearly_rate = (yearly_resistant / yearly_counts * 100).round(1)

# Plot
fig = plt.figure(figsize=(10,6))

# Total samples (base bar)
plt.bar(yearly_counts.index, yearly_counts.values,
        color='lightgray', label='Total Samples', width=0.8)

# Resistant samples (overlay bar)
plt.bar(yearly_resistant.index, yearly_resistant.values,
        color='tomato', label='Resistant Samples', width=0.6)

# Add % labels on top of total bars
for x, y, pct in zip(yearly_counts.index, yearly_counts.values, yearly_rate.values):
    plt.text(x, y + 20, f'{pct}%', ha='center', va='bottom', fontsize=9)

plt.title("Total vs Resistant Samples per Year")
plt.xlabel("Year")
plt.ylabel("Number of Samples")
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()
fig.savefig(os.path.join(plotdir, 'total_vs_resistant_samples_per_year.png'), dpi=300)

##pretty imbalanced dataset - we will have to account for that when training our models