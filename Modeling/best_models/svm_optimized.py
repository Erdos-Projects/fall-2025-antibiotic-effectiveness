# -*- coding: utf-8 -*-
"""SVMOptimized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tasZy9cyRWt0wBmKV1P2WNOcsT41KQQX
"""

##more rigorous hyperparamter searching specifically for SVM based on previous results
##designed for comparison to logistic regression script to determine final best model

##script takes about 10 minutes to run

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from re import X
from sklearn.model_selection import (train_test_split, StratifiedKFold,
                                      cross_val_score, cross_val_predict, cross_validate)
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibrationDisplay, CalibratedClassifierCV, FrozenEstimator, calibration_curve
from sklearn.metrics import (classification_report, accuracy_score, make_scorer,
                             confusion_matrix, average_precision_score, f1_score,
                             roc_auc_score, precision_score, recall_score,
                             brier_score_loss, RocCurveDisplay, PrecisionRecallDisplay, get_scorer,
                             ConfusionMatrixDisplay)
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from scipy.stats import loguniform
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

##find dataframe
from google.colab import drive
drive.mount('/content/drive')
folder_path = '/content/drive/MyDrive/DS_Project_2025/ARMD_data/Final_dataframe/'

##model setup

antibiotics = ['Gentamicin', 'Trimethoprim_Sulfamethoxazole', 'Ciprofloxacin',
                'Ampicillin', 'Cefazolin','Nitrofurantoin','Piperacillin_Tazobactam',
                'Levofloxacin', 'Ceftriaxone']

# 5 kfold cv
random_state = 312
# hyperparameter otimization CV
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)

# build pipeline
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=random_state)),
    ('svc', LinearSVC(dual='auto',max_iter=50000,random_state=random_state)),
])

# model parameters
param_grid = [
    {'svc__C':loguniform(0.3,3),
     'svc__loss':['squared_hinge'],
     'svc__penalty':['l1','l2']}
]

# Custom scorer for False Negative Rate
def fnr(y_test, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    fnr = fn / (fn + tp)
    return fnr

# Scoring dictionary
scoring = {
    'accuracy': 'accuracy',
    'precision': make_scorer(precision_score, pos_label=1),
    'recall': make_scorer(recall_score, pos_label=1),
    'roc_auc': 'roc_auc',
    'fnr': make_scorer(fnr),
    'f1_macro': make_scorer(f1_score, average='macro', pos_label=1),
    'f1_weighted': make_scorer(f1_score, average='weighted', pos_label=1)
}

savedir = '/content/drive/MyDrive/DS_Project_2025/Modeling/Support_vector_machines/optimized_SVM/'

from ast import Raise
metric_results = []
for i in antibiotics:
  print('\n')
  print(f"==== Antibiotic: {i} ====")
  train_data = pd.read_csv(f'{folder_path}{i}_train_data.csv')
  X_temp1 = train_data.drop(i, axis=1)
  X_temp2 = X_temp1.drop("anon_id",axis=1)
  X_all = X_temp2.drop("Year",axis=1)
  y_all_raw = train_data[i]
  y_all = (y_all_raw==2.0).astype(int)

  #need a separate test set for the calibration curve
  X_train, X_train_test, y_train, y_train_test = train_test_split(X_all, y_all, shuffle=True, random_state=312)

  #set up randomized hyperparameter search
  grid = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_grid,
    cv=kf,
    scoring=scoring,
    n_jobs=1,
    n_iter=100,
    random_state=random_state,
    refit='f1_weighted',
    error_score='raise'
  )

  #identify optimal hyperparameters
  grid.fit(X_train, y_train)

  #print optimal hyperparameters and scores
  print("Best parameters:", grid.best_params_)
  print("Best score:", grid.best_score_)

  #get the scores at each cross-validation split and print them
  results = grid.cv_results_

  print("\nAverage CV Metrics for Best Model:")

  scores = {"Antibiotic" : i}
  for metric in scoring.keys():
      mean_score = results[f'mean_test_{metric}'][grid.best_index_]
      std_score = results[f'std_test_{metric}'][grid.best_index_]
      scores[metric] = (mean_score, std_score)
      print(f"{metric}: {mean_score:.3f} ± {std_score:.3f}")

  metric_results.append(scores)

  #identify best model
  best_model = grid.best_estimator_
  #freeze best model so it doesn't do inner cross-validation for calibration
  #but instead does calibration on the already best fit model
  froze_svc = FrozenEstimator(best_model)
  cal_sig_svc = CalibratedClassifierCV(froze_svc, method='sigmoid')
  cal_iso_svc = CalibratedClassifierCV(froze_svc, method='isotonic')
  cal_sig_svc.fit(X_train,y_train)
  cal_iso_svc.fit(X_train,y_train)

  #create plots of different scores
  fig, ax = plt.subplots(1, 2, figsize=(10, 4))

  PrecisionRecallDisplay.from_estimator(best_model, X_train_test, y_train_test, ax=ax[0])
  ax[0].set_title("Precision-Recall Curve")

  CalibrationDisplay.from_estimator(cal_sig_svc, X_train_test, y_train_test, name='Sigmoid Calibrated LinearSVC', ax=ax[1])
  CalibrationDisplay.from_estimator(cal_iso_svc, X_train_test, y_train_test, name='Isotonic Calibrated LinearSVC', ax=ax[1])
  ax[1].set_title("Calibration Plot")

  plt.tight_layout()
  plt.savefig(os.path.join(savedir, f'{i}_calibration_PR_curve.png'), dpi=300)
  plt.show()
  plt.close()

results_df = pd.DataFrame(metric_results)
results_df.to_csv(os.path.join(savedir, 'optimized_svc_metric_scores.csv'), index=False)

###now we test model on the actual test hold out data
train_metric_results = []
final_metric_results = []
compare_metric_results = []
feature_importance_results = []

for i in antibiotics:
  print('\n')
  print(f"==== Antibiotic: {i} ====")

  ##test data
  test_data = pd.read_csv(f'{folder_path}{i}_test_data.csv')
  X_final_temp1 = test_data.drop(i, axis=1)
  X_final_temp2 = X_final_temp1.drop("anon_id",axis=1)
  X_final_test = X_final_temp2.drop("Year",axis=1)
  y_final_test_raw = test_data[i]
  y_final_test = (y_final_test_raw==2.0).astype(int)

  ##original train data
  train_data = pd.read_csv(f'{folder_path}{i}_train_data.csv')
  X_temp1 = train_data.drop(i, axis=1)
  X_temp2 = X_temp1.drop("anon_id",axis=1)
  X_train = X_temp2.drop("Year",axis=1)
  y_train_raw = train_data[i]
  y_train = (y_all_raw==2.0).astype(int)

  #set up randomized hyperparameter search
  new_grid = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_grid,
    cv=kf,
    scoring=scoring,
    n_jobs=1,
    n_iter=100,
    random_state=random_state,
    refit='f1_weighted',
    error_score='raise'
  )

  #identify optimal hyperparameters
  new_grid.fit(X_train, y_train)

  #print optimal hyperparameters and scores
  # print("Best parameters, train:", new_grid.best_params_)
  # print("Best score:", new_grid.best_score_)

  new_best_model = new_grid.best_estimator_

  #give training set metrics
  cv_scores = {"Antibiotic" : i}
  for metric in scoring.keys():
      mean_score = results[f'mean_test_{metric}'][new_grid.best_index_]
      std_score = results[f'std_test_{metric}'][new_grid.best_index_]
      cv_scores[metric] = (mean_score, std_score)
      # print(f"{metric}: {mean_score:.3f} ± {std_score:.3f}")

  train_metric_results.append(cv_scores)

  #give testing set metrics
  test_set_scores = {"Antibiotic" : i}
  for metric_name, metric_func in scoring.items():
      scorer = get_scorer(metric_func)
      test_set_scores[metric_name] = scorer(new_best_model, X_final_test, y_final_test)


  # Store results
  final_metric_results.append(test_set_scores)




  #freeze estimator for calibrated classifier
  froze_new_svc = FrozenEstimator(new_best_model)
  X_res, y_res = new_best_model.named_steps['smote'].fit_resample(X_train, y_train)
  cal_sig = CalibratedClassifierCV(froze_new_svc,method='sigmoid')

  cal_sig.fit(X_res, y_res)

  ##want to compare uncalibrated scores, to both types of calibrated ones
  uncal_scores = new_best_model.decision_function(X_final_test)
  uncal_probs = (uncal_scores - uncal_scores.min()) / (uncal_scores.max() - uncal_scores.min())
  cal_sig_probs = cal_sig.predict_proba(X_final_test)[:,1]

  true_uncal, pred_uncal = calibration_curve(y_final_test,uncal_probs,strategy='quantile',n_bins=10)
  true_sig, pred_sig = calibration_curve(y_final_test,cal_sig_probs,strategy='quantile',n_bins=10)




  #also want to plot confusion matrices
  y_pred = new_best_model.predict(X_final_test)

  ##create confusion matrix
  fig, ax = plt.subplots(1,2)

  cm = confusion_matrix(y_final_test, y_pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  disp.plot(ax=ax[0], colorbar=False)
  ax[0].set_title('Raw SVM')

  cm1 = confusion_matrix(y_final_test, cal_sig.predict(X_final_test))
  disp = ConfusionMatrixDisplay(confusion_matrix=cm1)
  disp.plot(ax=ax[1], colorbar=False)
  ax[1].set_title('Sigmoid Calibrated')

  fig.suptitle(f'{i} Confusion Matrices')

  plt.tight_layout()
  plt.show()
  fig.savefig(os.path.join(savedir, f'{i}_svm_final_test_confusion_matrices.png'), dpi=300)


  fig, ax = plt.subplots(1,1)
  ax.plot(pred_uncal, true_uncal, label='Uncalibrated')
  ax.plot(pred_sig, true_sig, label='Sigmoid Calibrated')
  ax.plot([0,1],[0,1], color='navy', linestyle='--', label="Perfect Calibration")
  ax.set_title(f'{i} Calibration Curve')
  ax.legend()

  plt.tight_layout()
  plt.show()
  fig.savefig(os.path.join(savedir, f'{i}_svm_final_test_calibration.png'), dpi=300)


  #compare calibrated metrics to uncalibrated metric

  compare_result_entry = {"Antibiotic" : i}
  compare_result_entry["brier_pre"] = brier_score_loss(y_final_test, uncal_probs)
  compare_result_entry["brier_post_sig"] = brier_score_loss(y_final_test, cal_sig_probs)

  for metric_name, scorer_key in scoring.items():
      scorer = get_scorer(scorer_key)
      compare_result_entry[f"{metric_name}_pre"] = scorer(new_best_model, X_final_test, y_final_test)
      compare_result_entry[f"{metric_name}_post_sig"] = scorer(cal_sig, X_final_test, y_final_test)
      # print(f"{metric_name}_pre: {compare_result_entry[f'{metric_name}_pre']:.3f}")
      # print(f"{metric_name}_post_sig: {compare_result_entry[f'{metric_name}_post_sig']:.3f}")

  print("\n")
  print(f"Final Test Results: {new_grid.best_params_}")
  for k, v in compare_result_entry.items():
      if k != "Antibiotic":
          print(f"{k}: {v:.3f}")
  print("\n")

  compare_metric_results.append(compare_result_entry)


  coefficients = new_best_model.named_steps['svc'].coef_[0]
  feature_importance_df = pd.DataFrame({'feature':X_final_test.columns, 'importance':np.abs(coefficients)})
  sorted_df = feature_importance_df.sort_values(by='importance',ascending=False)

  fig, ax = plt.subplots(1,1)
  ax.barh(sorted_df['feature'],sorted_df['importance'])
  ax.set_xlabel("Absolute Coefficient Value")
  ax.set_ylabel("Feature")
  ax.set_title(f"Feature Importance for SVM on {i} Data")
  plt.gca().invert_yaxis()

  plt.tight_layout()
  plt.show()
  fig.savefig(os.path.join(savedir, f'{i}_svm_feature_importance.png'), dpi=300)

final_results_df = pd.DataFrame(compare_metric_results)
final_results_df.to_csv(os.path.join(savedir, 'optimized_svc_final_test_metric_scores.csv'), index=False)