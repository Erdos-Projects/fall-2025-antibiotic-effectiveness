# -*- coding: utf-8 -*-
"""SVM_parameter_fitting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RoXnd19vFvIDcK2dDVLLkjJjT14hyz7N
"""

##Original testing for svm
##this script is mostly experimentation on
# 1) how to parameter search (end choice RandomSearchCV)
# 2) what best C paramater looks like depending on the fitting measure when using RandomSearchCV (and is it consistent across models)
# 3) how to do calibration on a best fit model for a model that doesn't have in-built scores

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, brier_score_loss, average_precision_score
from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC, LinearSVC
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV, calibration_curve, FrozenEstimator
from scipy.stats import uniform

##originally working in google colab
##change folder_path to appropriate path
##ex: folder_path = '.../fall-2025-antibiotic-effectiveness/Data/final_dataframes/

from google.colab import drive
drive.mount('/content/drive')
folder_path = '/content/drive/MyDrive/DS_Project_2025/ARMD_data/Final_dataframe/'

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, precision_score, recall_score

##create pipeline
##using LinearSVC due to the extensive time it took to identify hyperparameters with SVC (upward of five hours on personal computer)
##two benefits:
#   1) much much faster
#   2) eventual goal of feature importance, which can be easily determined from linearsvc coef_
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('svc', LinearSVC(class_weight='balanced',max_iter=10000))
])

##
# 5 kfold cv
random_state = 312
# hyperparameter otimization CV
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)

# Define FNR scorer (since sklearn doesnâ€™t have one built-in)
# Custom scorer for False Negative Rate
def fnr(y_test, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    fnr = fn / (fn + tp)
    return fn / (fn + tp) if (fn + tp) > 0 else 0.0

# custom scorer
fnr_scorer = make_scorer(fnr, greater_is_better=False)

scoring_all = {
    'accuracy': 'accuracy',
    'f1': 'f1',
    'roc_auc': 'roc_auc',
    'precision': 'precision',
    'recall': 'recall',
    'fnr': fnr_scorer
}


param_dist = {
    'svc__C': uniform(0.1, 10)
}


antibiotics = ['Gentamicin', 'Trimethoprim_Sulfamethoxazole', 'Ciprofloxacin',
                'Ampicillin', 'Cefazolin','Nitrofurantoin','Piperacillin_Tazobactam',
                'Levofloxacin', 'Ceftriaxone']

##saving results directory
savedir = "/content/drive/MyDrive/DS_Project_2025/Modeling/Support_vector_machines/svm_experimentation_results/"

## each search takes approximatel ~10 seconds, total runtime for each search
## accross all antibiotics is 1.5 minutes

#start with optimizing based on accuracy
rand_acc = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_dist,
    scoring='accuracy',
    cv=kf,
    n_iter=50,
    n_jobs=1,
    random_state=312
)

results_based_on_accuracy = []

for j in antibiotics:
  # print(f"Antibiotic: {j}")

  #read in data
  train_data = pd.read_csv(f'{folder_path}{j}_train_data.csv')
  X_train = train_data.drop(j, axis=1)
  y_train = train_data[j]
  y_train_binary = (y_train==2.0).astype(int)
  rand_acc.fit(X_train,y_train_binary)

  #save resuls
  row = rand_acc.best_params_.copy()
  row['best_accuracy_score'] = rand_acc.best_score_
  row['antibiotic'] = j
  results_based_on_accuracy.append(row)

  # print("Best parameters based on accuracy:", rand_acc.best_params_)
  # print("Best accuracy score:", rand_acc.best_score_)

df_accuracy_best = pd.DataFrame(results_based_on_accuracy)
print(df_accuracy_best)
df_accuracy_best.to_csv(f'{savedir}best_fit_based_on_accuracy.csv',index=False)

#optimization based on f1
rand_f1 = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_dist,
    scoring='f1',
    cv=kf,
    n_iter=50,
    n_jobs=1,
    random_state=312
)

results_based_on_f1 = []


for j in antibiotics:
  # print(f"Antibiotic: {j}")
  train_data = pd.read_csv(f'{folder_path}{j}_train_data.csv')
  X_train = train_data.drop(j, axis=1)
  y_train = train_data[j]
  y_train_binary = (y_train==2.0).astype(int)
  rand_f1.fit(X_train,y_train_binary)

  row = rand_f1.best_params_.copy()
  row['best_f1_score'] = rand_f1.best_score_
  row['antibiotic'] = j
  results_based_on_f1.append(row)
  # print("Best parameters based on f1:", rand_f1.best_params_)
  # print("Best f1 score:", rand_f1.best_score_)

df_f1_best = pd.DataFrame(results_based_on_f1)
print(df_f1_best)
df_accuracy_best.to_csv(f'{savedir}best_fit_based_on_f1.csv',index=False)

#optimization based on average precision
rand_ave_prec = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_dist,
    scoring='average_precision',
    cv=kf,
    n_iter=50,
    n_jobs=1,
    random_state=312
)

results_based_on_ave_prec = []


for j in antibiotics:
  # print(f"Antibiotic: {j}")
  train_data = pd.read_csv(f'{folder_path}{j}_train_data.csv')
  X_train = train_data.drop(j, axis=1)
  y_train = train_data[j]
  y_train_binary = (y_train==2.0).astype(int)
  rand_ave_prec.fit(X_train,y_train_binary)

  row = rand_ave_prec.best_params_.copy()
  row['best_average_precision_score'] = rand_ave_prec.best_score_
  row['antibiotic'] = j
  results_based_on_ave_prec.append(row)
  # print("Best parameters based on average_precision:", rand_ave_prec.best_params_)
  # print("Best average_precision score:", rand_ave_prec.best_score_)

df_ave_prec_best = pd.DataFrame(results_based_on_ave_prec)
print(df_ave_prec_best)
df_accuracy_best.to_csv(f'{savedir}best_fit_based_on_average_precision.csv',index=False)

#results based on precision
rand_prec = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_dist,
    scoring='precision',
    cv=kf,
    n_iter=50,
    n_jobs=1,
    random_state=312
)

results_based_on_prec = []


for j in antibiotics:
  # print(f"Antibiotic: {j}")
  train_data = pd.read_csv(f'{folder_path}{j}_train_data.csv')
  X_train = train_data.drop(j, axis=1)
  y_train = train_data[j]
  y_train_binary = (y_train==2.0).astype(int)
  rand_prec.fit(X_train,y_train_binary)

  row = rand_prec.best_params_.copy()
  row['best_precision_score'] = rand_prec.best_score_
  row['antibiotic'] = j
  results_based_on_prec.append(row)
  # print("Best parameters based on precision:", rand_prec.best_params_)
  # print("Best precision score:", rand_prec.best_score_)

df_prec_best = pd.DataFrame(results_based_on_prec)
print(df_prec_best)
df_accuracy_best.to_csv(f'{savedir}best_fit_based_on_precision.csv',index=False)

#optimization based on recall
rand_recall = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_dist,
    scoring='recall',
    cv=kf,
    n_iter=50,
    n_jobs=1,
    random_state=312
)

results_based_on_recall = []


for j in antibiotics:
  #print(f"Antibiotic: {j}")
  train_data = pd.read_csv(f'{folder_path}{j}_train_data.csv')
  X_train = train_data.drop(j, axis=1)
  y_train = train_data[j]
  y_train_binary = (y_train==2.0).astype(int)
  rand_recall.fit(X_train,y_train_binary)

  row = rand_recall.best_params_.copy()
  row['best_recall_score'] = rand_recall.best_score_
  row['antibiotic'] = j
  results_based_on_recall.append(row)
  # print("Best parameters based on recall:", rand_recall.best_params_)
  # print("Best recall score:", rand_recall.best_score_)

df_recall_best = pd.DataFrame(results_based_on_recall)
print(df_recall_best)
df_accuracy_best.to_csv(f'{savedir}best_fit_based_on_recall.csv',index=False)

#optimization with false negative rate
rand_fnr = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_dist,
    scoring=fnr_scorer,
    cv=kf,
    n_iter=50,
    n_jobs=1,
    random_state=312
)

results_based_on_fnr = []


for j in antibiotics:
  # print(f"Antibiotic: {j}")
  train_data = pd.read_csv(f'{folder_path}{j}_train_data.csv')
  X_train = train_data.drop(j, axis=1)
  y_train = train_data[j]
  y_train_binary = (y_train==2.0).astype(int)
  rand_fnr.fit(X_train,y_train_binary)

  row = rand_fnr.best_params_.copy()
  row['best_fnr_score'] = rand_fnr.best_score_
  row['antibiotic'] = j
  results_based_on_fnr.append(row)
  # print("Best parameters based on fnr:", rand_fnr.best_params_)
  # print("Best fnr score:", rand_fnr.best_score_)

df_fnr_best = pd.DataFrame(results_based_on_fnr)
print(df_fnr_best)
df_accuracy_best.to_csv(f'{savedir}best_fit_based_on_fnr.csv',index=False)

##why is fnr negative?

##calibration curves

##need to use outer and innner cv because otherwise the calibration curves
##are trained on the same data they are tested on
##and they are very obviously not correct

##we manually loop through an outer k fold
##and then use an inner cv for hyperparameter fitting

##
from sklearn.utils.class_weight import compute_class_weight
from sklearn.calibration import calibration_curve
import os


for anti in antibiotics:
  print(f"Antibiotic: {anti}")

  anti_data = pd.read_csv(f'{folder_path}{anti}_train_data.csv')
  X_all = anti_data.drop(anti,axis=1)
  y_all = anti_data[anti]
  y_all_binary = (y_all==2.0).astype(int)

  avebr_scores = []
  y_true_all = []
  y_prob_all = []
  outer_scores = []

  for train_idx, test_idx in kf.split(X_all, y_all_binary):
      X_train, X_test = X_all.iloc[train_idx], X_all.iloc[test_idx]
      y_train, y_test = y_all_binary.iloc[train_idx], y_all_binary.iloc[test_idx]

      inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=312)

      search = RandomizedSearchCV(
          estimator=pipe,
          param_distributions=param_dist,
          scoring = 'average_precision',
          cv=inner_cv,
          n_iter=50,
          random_state=312
      )

      search.fit(X_train, y_train)

      best_model = search.best_estimator_
      froze = FrozenEstimator(best_model)
      calibrator = CalibratedClassifierCV(froze, method='isotonic')
      calibrator.fit(X_train, y_train)


      y_probs = calibrator.predict_proba(X_test)[:, 1]
      y_true_all.extend(y_test)
      y_prob_all.extend(y_probs)

      brier = brier_score_loss(y_test,y_probs)
      avebr_scores.append(brier)


  # Calibration curve
  prob_true, prob_pred = calibration_curve(y_true_all, y_prob_all, n_bins=10, strategy='uniform')
  brier_ave = np.mean(avebr_scores)

  fig = plt.figure(figsize=(8, 6))
  plt.plot(prob_pred, prob_true, marker='o', label='Calibrated LinearSVC')
  plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')
  plt.xlabel("Mean Predicted Probability")
  plt.ylabel("Fraction of Positives")
  plt.title(f"{anti} Calibration Plot, Brier Score Loss {brier_ave}")
  plt.legend()
  plt.grid()
  plt.show()

  fig.savefig(os.path.join(savedir, f'{anti}_calibration_curve.png'), dpi=300)