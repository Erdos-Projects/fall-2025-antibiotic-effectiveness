# -*- coding: utf-8 -*-
"""HyperparameterOptimizationAllModels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_4dFJoHMwsNPBfF1UL9KZoHDyFld53pM
"""

##Hyperparameter Optimization for All Models

##By: Mustafain Ali, Tinghao Huang, Dominique Hughes, Chiara Mattamira, Haejun (Stella) Oh

##import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import seaborn as sns
from sklearn.metrics import brier_score_loss, average_precision_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, make_scorer
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate, cross_val_predict, GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC, LinearSVC
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV, calibration_curve, FrozenEstimator
from scipy.stats import uniform
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.dummy import DummyClassifier
from xgboost import XGBClassifier
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
import plotly.graph_objects as go

##find dataframe

##originally working in google colab
##change folder_path to appropriate path
##ex: folder_path = '.../fall-2025-antibiotic-effectiveness/Data/final_dataframes/
from google.colab import drive
drive.mount('/content/drive')
folder_path = '/content/drive/MyDrive/DS_Project_2025/ARMD_data/Final_dataframe/'

##Set up for future code:

#antibiotics to loop through
antibiotics = ['Gentamicin', 'Trimethoprim_Sulfamethoxazole', 'Ciprofloxacin',
                'Ampicillin', 'Cefazolin','Nitrofurantoin','Piperacillin_Tazobactam',
                'Levofloxacin', 'Ceftriaxone']

#outer and inner kfold set up
random_state = 312
outer_kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=random_state)
inner_kf = StratifiedKFold(n_splits=3,shuffle=True,random_state=random_state)

#define scorers
# false negative rate
def false_negative_rate(y_test, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    fnr = fn / (fn + tp)
    return fnr

fnr_scorer = make_scorer(false_negative_rate, greater_is_better=False)
f1_weighted = make_scorer(f1_score, average='weighted', pos_label=1)

scoring = {
    'accuracy': 'accuracy',
    'f1': 'f1',
    'pr_auc': 'average_precision',
    'precision': 'precision',
    'recall': 'recall',
}

#pipelines and parameters for tuning

pipe_log = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=312)),
    ('log', LogisticRegression(class_weight='balanced',max_iter=10000))
])

pg_log = {
    'log__C': uniform(0.1,10),
    'log__solver': ['liblinear','lbfgs']
}


pipe_svc = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=312)),
    ('svc', LinearSVC(class_weight='balanced',max_iter=10000))
])

pg_svc = {
    'svc__C': uniform(0.1,10)
}

pipe_knn = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=312)),
    ('knn', KNeighborsClassifier())
])

pg_knn = {
    'knn__n_neighbors': list(range(1,101,2))
}

pipe_rf = Pipeline([
    ('smote', SMOTE(random_state=312)),
    ('rf', RandomForestClassifier(min_samples_split=2,
                                  min_samples_leaf=1,
                                  class_weight='balanced',
                                  random_state=random_state))
])

pg_rf = {
    'rf__n_estimators': list(range(50,200,1)),

}

pipe_xgb = Pipeline([
    ('smote', SMOTE(random_state=312)),
    ('xgb', XGBClassifier(max_depth=5,
                          subsample=0.8,
                          colsample_bytree=0.8,
                          eval_metric='logloss',
                          random_state=random_state))
])

pg_xgb = {
    'xgb__n_estimators': list(range(50,200,1)),
    'xgb__learning_rate': uniform(0.01,0.9)
}


#function to help with model evaluation
def evaluate_model(y_test, y_preds):
  acc = accuracy_score(y_test, y_preds)
  f1 = f1_score(y_test, y_preds)
  pr_auc = average_precision_score(y_test, y_preds)
  prec = precision_score(y_test, y_preds,zero_division=0)
  recall = recall_score(y_test, y_preds)
  fnr = false_negative_rate(y_test, y_preds)
  # brier = brier_score_loss(y_test, y_probs)


  return [acc, f1, pr_auc, prec, recall, fnr]

##Create optimized solution for all models.
##We use f1_weighted as the optimization measure for hyperparameter tuning
##takes about 2 hours to identify best fit for each model

#loop through each antibiotic

log_results_summary = {}
svc_results_summary = {}
knn_results_summary = {}
rf_results_summary = {}
xgb_results_summary = {}
dummy_results_summary = {}


log_y_predictions = {}
svc_y_predictions = {}
knn_y_predictions = {}
rf_y_predictions = {}
xgb_y_predictions = {}
dummy_y_predictions = {}
true_y = {}

for anti in antibiotics:
  print(f"Antibiotic: {anti}")
  anti_data = pd.read_csv(f'{folder_path}{anti}_train_data.csv') #read in data
  X_temp1 = anti_data.drop(anti,axis=1)#drop antibiotic resistance from X data
  X_temp2 = X_temp1.drop("Year", axis=1) #drop year
  X_temp3 = X_temp2.drop("anon_id", axis=1) #drop anon_id
  if "Date" in X_temp3.columns:
    X_all = X_temp3.drop("Date",axis=1) #drop date
  else: X_all = X_temp3 #drop anon_id
  y_all = anti_data[anti] #keep antibiotic resistance for y
  y_all_binary = (y_all==2.0).astype(int) #change 1/2 labels to 0/1 for classification

  #storing info for calibration curves
  y_true_all = []

  #storing information for dummy classifier
  dummy_acc_scores = []
  dummy_f1_scores = []
  dummy_pr_auc_scores = []
  dummy_prec_scores = []
  dummy_recall_scores = []
  dummy_fnr_scores = []
  # dummy_brier_scores = []
  # dummy_y_prob_all = []
  dummy_y_pred_all = []

  #storing information for logistic regression
  log_acc_scores = []
  log_f1_scores = []
  log_pr_auc_scores = []
  log_prec_scores = []
  log_recall_scores = []
  log_fnr_scores = []
  # log_brier_scores = []
  # log_y_prob_all = []
  log_y_pred_all = []

  #storing information for support vector machines
  svc_acc_scores = []
  svc_f1_scores = []
  svc_pr_auc_scores = []
  svc_prec_scores = []
  svc_recall_scores = []
  svc_fnr_scores = []
  # svc_brier_scores = []
  # svc_y_prob_all = []
  svc_y_pred_all = []

  #storing information for knn
  knn_acc_scores = []
  knn_f1_scores = []
  knn_pr_auc_scores = []
  knn_prec_scores = []
  knn_recall_scores = []
  knn_fnr_scores = []
  # knn_brier_scores = []
  # knn_y_prob_all = []
  knn_y_pred_all = []

  #storing information for random forest
  rf_acc_scores = []
  rf_f1_scores = []
  rf_pr_auc_scores = []
  rf_prec_scores = []
  rf_recall_scores = []
  rf_fnr_scores = []
  # rf_brier_scores = []
  # rf_y_prob_all = []
  rf_y_pred_all = []

  #storing information for xgboost
  xgb_acc_scores = []
  xgb_f1_scores = []
  xgb_pr_auc_scores = []
  xgb_prec_scores = []
  xgb_recall_scores = []
  xgb_fnr_scores = []
  # xgb_brier_scores = []
  # xgb_y_prob_all = []
  xgb_y_pred_all = []


  #loop through the outer folds
  for train_idx, test_idx in outer_kf.split(X_all,y_all_binary):
    #split data based on the outer k folds
    X_train, X_test = X_all.iloc[train_idx], X_all.iloc[test_idx]
    y_train, y_test = y_all_binary.iloc[train_idx], y_all_binary.iloc[test_idx]


    #inner cv hyperparameter tuning
    #set up RandomizedSearch for each model
    log_search = RandomizedSearchCV(
        estimator=pipe_log,
        param_distributions=pg_log,
        scoring = f1_weighted,
        cv=inner_kf,
        n_iter=50,
        random_state=312
    )

    svc_search = RandomizedSearchCV(
        estimator=pipe_svc,
        param_distributions=pg_svc,
        scoring = f1_weighted,
        cv=inner_kf,
        n_iter=50,
        random_state=random_state
    )

    knn_search = RandomizedSearchCV(
        estimator=pipe_knn,
        param_distributions=pg_knn,
        scoring = f1_weighted,
        cv=inner_kf,
        n_iter=50,
        random_state=random_state
    )

    rf_search = RandomizedSearchCV(
        estimator=pipe_rf,
        param_distributions=pg_rf,
        scoring = f1_weighted,
        cv=inner_kf,
        n_iter=50,
        random_state=random_state
    )

    xgb_search = RandomizedSearchCV(
        estimator=pipe_xgb,
        param_distributions=pg_xgb,
        scoring= f1_weighted,
        cv=inner_kf,
        n_iter=50,
        random_state=random_state
    )

    ##save y_true data
    y_true_all.extend(y_test)

    #identify best fit for the model
    log_search.fit(X_train, y_train)
    best_log = log_search.best_estimator_
    lr_preds = best_log.predict(X_test)
    log_y_pred_all.extend(lr_preds)

    svc_search.fit(X_train, y_train)
    best_svc = svc_search.best_estimator_
    svc_preds = best_svc.predict(X_test)
    svc_y_pred_all.extend(svc_preds)

    knn_search.fit(X_train, y_train)
    best_knn = knn_search.best_estimator_
    knn_preds = best_knn.predict(X_test)
    knn_y_pred_all.extend(knn_preds)

    rf_search.fit(X_train, y_train)
    best_rf = rf_search.best_estimator_
    rf_preds = best_rf.predict(X_test)
    rf_y_pred_all.extend(rf_preds)

    xgb_search.fit(X_train, y_train)
    best_xgb = xgb_search.best_estimator_
    xgb_preds = best_xgb.predict(X_test)
    xgb_y_pred_all.extend(xgb_preds)


    dummy = DummyClassifier(strategy='stratified', random_state=random_state)
    dummy.fit(X_train, y_train)
    dummy_preds = dummy.predict(X_test)
    dummy_y_pred_all.extend(dummy_preds)

    ##calculate and save metrics
    log_outputs = evaluate_model(y_test, lr_preds)
    svc_outputs = evaluate_model(y_test, svc_preds)
    knn_outputs = evaluate_model(y_test, knn_preds)
    rf_outputs = evaluate_model(y_test, rf_preds)
    xgb_outputs = evaluate_model(y_test, xgb_preds)
    dummy_outputs = evaluate_model(y_test, dummy_preds)

    dummy_acc_scores.append(dummy_outputs[0])
    dummy_f1_scores.append(dummy_outputs[1])
    dummy_pr_auc_scores.append(dummy_outputs[2])
    dummy_recall_scores.append(dummy_outputs[3])
    dummy_prec_scores.append(dummy_outputs[4])
    dummy_fnr_scores.append(dummy_outputs[5])
    # dummy_brier_scores.append(dummy_outputs[6])

    log_acc_scores.append(log_outputs[0])
    log_f1_scores.append(log_outputs[1])
    log_pr_auc_scores.append(log_outputs[2])
    log_prec_scores.append(log_outputs[3])
    log_recall_scores.append(log_outputs[4])
    log_fnr_scores.append(log_outputs[5])
    # log_brier_scores.append(log_outputs[6])

    svc_acc_scores.append(svc_outputs[0])
    svc_f1_scores.append(svc_outputs[1])
    svc_pr_auc_scores.append(svc_outputs[2])
    svc_prec_scores.append(svc_outputs[3])
    svc_recall_scores.append(svc_outputs[4])
    svc_fnr_scores.append(svc_outputs[5])
    # svc_brier_scores.append(svc_outputs[6])

    knn_acc_scores.append(knn_outputs[0])
    knn_f1_scores.append(knn_outputs[1])
    knn_pr_auc_scores.append(knn_outputs[2])
    knn_prec_scores.append(knn_outputs[3])
    knn_recall_scores.append(knn_outputs[4])
    knn_fnr_scores.append(knn_outputs[5])
    # knn_brier_scores.append(knn_outputs[6])

    rf_acc_scores.append(rf_outputs[0])
    rf_f1_scores.append(rf_outputs[1])
    rf_pr_auc_scores.append(rf_outputs[2])
    rf_prec_scores.append(rf_outputs[3])
    rf_recall_scores.append(rf_outputs[4])
    rf_fnr_scores.append(rf_outputs[5])
    # rf_brier_scores.append(rf_outputs[6])

    xgb_acc_scores.append(xgb_outputs[0])
    xgb_f1_scores.append(xgb_outputs[1])
    xgb_pr_auc_scores.append(xgb_outputs[2])
    xgb_prec_scores.append(xgb_outputs[3])
    xgb_recall_scores.append(xgb_outputs[4])
    xgb_fnr_scores.append(xgb_outputs[5])
    # xgb_brier_scores.append(xgb_outputs[6])







  ##record results
  log_results_summary[anti] = [np.mean(log_acc_scores),
                               np.mean(log_f1_scores),
                               np.mean(log_pr_auc_scores),
                               np.mean(log_prec_scores),
                               np.mean(log_recall_scores)]

  svc_results_summary[anti] = [np.mean(svc_acc_scores),
                               np.mean(svc_f1_scores),
                               np.mean(svc_pr_auc_scores),
                               np.mean(svc_prec_scores),
                               np.mean(svc_recall_scores)]

  knn_results_summary[anti] = [np.mean(knn_acc_scores),
                               np.mean(knn_f1_scores),
                               np.mean(knn_pr_auc_scores),
                               np.mean(knn_prec_scores),
                               np.mean(knn_recall_scores)]

  rf_results_summary[anti] = [np.mean(rf_acc_scores),
                               np.mean(rf_f1_scores),
                               np.mean(rf_pr_auc_scores),
                               np.mean(rf_prec_scores),
                               np.mean(rf_recall_scores)]

  xgb_results_summary[anti] = [np.mean(xgb_acc_scores),
                               np.mean(xgb_f1_scores),
                               np.mean(xgb_pr_auc_scores),
                               np.mean(xgb_prec_scores),
                               np.mean(xgb_recall_scores)]

  dummy_results_summary[anti] = [np.mean(dummy_acc_scores),
                               np.mean(dummy_f1_scores),
                               np.mean(dummy_pr_auc_scores),
                               np.mean(dummy_prec_scores),
                               np.mean(dummy_recall_scores)]

  log_y_predictions[anti] = log_y_pred_all
  svc_y_predictions[anti] = svc_y_pred_all
  knn_y_predictions[anti] = knn_y_pred_all
  rf_y_predictions[anti] = rf_y_pred_all
  xgb_y_predictions[anti] = xgb_y_pred_all
  dummy_y_predictions[anti] = dummy_y_pred_all
  true_y[anti] = y_true_all

##optional to run - comment out if not needed
##due to run time of above, data is saved as pkl files
##in case we want to change plotting later we can just load the pkl files
##instead of running the above again for 2 hours

import pickle
import os

##change save_path to appropriate pkl files folder
save_path = '/content/drive/MyDrive/DS_Project_2025/Modeling/Support_vector_machines/pkl_files/'

file1 = "log_results_summary.pkl"
file2 = "svc_results_summary.pkl"
file3 = "knn_results_summary.pkl"
file4 = "rf_results_summary.pkl"
file5 = "xgb_results_summary.pkl"
file6 = "dummy_results_summary.pkl"

file7 = "log_y_predictions.pkl"
file8 = "svc_y_predictions.pkl"
file9 = "knn_y_predictions.pkl"
file10 = "rf_y_predictions.pkl"
file11 = "xgb_y_predictions.pkl"
file12 = "dummy_y_predictions.pkl"
file13 = "true_y.pkl"

with open(os.path.join(save_path,file1),'wb') as f:
  pickle.dump(log_results_summary,f)

with open(os.path.join(save_path,file2),'wb') as f:
  pickle.dump(svc_results_summary,f)

with open(os.path.join(save_path,file3),'wb') as f:
  pickle.dump(knn_results_summary,f)

with open(os.path.join(save_path,file4),'wb') as f:
  pickle.dump(rf_results_summary,f)

with open(os.path.join(save_path,file5),'wb') as f:
  pickle.dump(xgb_results_summary,f)

with open(os.path.join(save_path,file6),'wb') as f:
  pickle.dump(dummy_results_summary,f)

with open(os.path.join(save_path,file7),'wb') as f:
  pickle.dump(log_y_predictions,f)

with open(os.path.join(save_path,file8),'wb') as f:
  pickle.dump(svc_y_predictions,f)

with open(os.path.join(save_path,file9),'wb') as f:
  pickle.dump(knn_y_predictions,f)

with open(os.path.join(save_path,file10),'wb') as f:
  pickle.dump(rf_y_predictions,f)

with open(os.path.join(save_path,file11),'wb') as f:
  pickle.dump(xgb_y_predictions,f)

with open(os.path.join(save_path,file12),'wb') as f:
  pickle.dump(dummy_y_predictions,f)

with open(os.path.join(save_path,file13),'wb') as f:
  pickle.dump(true_y,f)

##plotting code

scores = ['accuracy', 'f1', 'pr_auc', 'precision', 'recall']

##change directory for saving radar and confusion matrices plots to match your own
plotdir = '/content/drive/MyDrive/DS_Project_2025/Modeling/Support_vector_machines/best_fit_plots_all_models/'


for anti in antibiotics:
  print(f"Antibiotic: {anti}")
  anti_data = pd.read_csv(f'{folder_path}{anti}_train_data.csv') #read in data
  y_train = anti_data[anti] #keep antibiotic resistance for y
  y_train_binary = (y_all==2.0).astype(int) #change 1/2 labels to 0/1 for classification

  fig = go.Figure()
  fig.add_trace(go.Scatterpolar(
      r = log_results_summary[anti],
      theta = scores,
      fill='toself',
      name='LogReg'
  ))
  fig.add_trace(go.Scatterpolar(
      r = svc_results_summary[anti],
      theta = scores,
      fill='toself',
      name='SVC'
  ))
  fig.add_trace(go.Scatterpolar(
      r = knn_results_summary[anti],
      theta = scores,
      fill='toself',
      name='KNN'
  ))
  fig.add_trace(go.Scatterpolar(
      r = rf_results_summary[anti],
      theta = scores,
      fill='toself',
      name='RanFor'
  ))
  fig.add_trace(go.Scatterpolar(
      r = xgb_results_summary[anti],
      theta = scores,
      fill='toself',
      name='XGB'
  ))
  fig.add_trace(go.Scatterpolar(
      r = dummy_results_summary[anti],
      theta = scores,
      fill='toself',
      name='Dummy'
  ))

  fig.update_layout(
      title = f'{anti} Best Fit All Model Scores',
      polar=dict(
          radialaxis=dict(
              visible=True,
              range=[0.1,1]
          )),
      showlegend=True
  )
  fig.show()
  fig.write_html(os.path.join(plotdir, f'{anti}_radar_plot.html'))



  cm_models = {
      "LogReg": log_y_predictions[anti],
      "SVC": svc_y_predictions[anti],
      "KNN": knn_y_predictions[anti],
      "RanFor": rf_y_predictions[anti],
      "XGB": xgb_y_predictions[anti],
      "Dummy": dummy_y_predictions[anti]
  }

  y_true = true_y[anti]

  fig, axes = plt.subplots(1,6,figsize=(15,4))

  for ax, (name, preds) in zip(axes, cm_models.items()):
    cm = confusion_matrix(y_true, preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(ax=ax, colorbar=False)
    ax.set_title(name)
    plt.suptitle(anti)



  plt.tight_layout()
  plt.show()
  fig.savefig(os.path.join(plotdir, f'{anti}_confusion_matrices.png'), dpi=300)