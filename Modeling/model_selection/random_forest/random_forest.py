# -*- coding: utf-8 -*-
"""Random_Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aFvtfdRLtp01u2CEa01x7tZNd1BDVKRJ
"""

import pandas as pd
import numpy as np

# Random Forest classifier
from sklearn.ensemble import RandomForestClassifier

# SMOTE for class imbalance
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline

# CV + metrics
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score,
    make_scorer
)

from scipy.stats import randint

from google.colab import drive
drive.mount('/content/drive')
dir = '/content/drive/MyDrive/DS_Project_2025/ARMD_data/Final_dataframe/'

# Loading the names for antibiotics, and reading their files.
antibiotics = ['Gentamicin', 'Trimethoprim_Sulfamethoxazole', 'Ciprofloxacin',
               'Ampicillin', 'Cefazolin', 'Nitrofurantoin', 'Piperacillin_Tazobactam',
               'Levofloxacin', 'Ceftriaxone']

# Fix a random state for reproducibility.
n_randstat = 312

# Collect results across all antibiotics
all_results = []

for anti in antibiotics:

    # Load and prepare data
    train_df = pd.read_csv(
        f'{dir}{anti}_train_data.csv'
    )

    X = train_df.drop(columns=[anti, 'Year', 'anon_id'])
    y = train_df[anti] - 1  # Convert {1,2} to {0,1}

    # Define base pipeline (no scaler needed for tree models)
    base_pipeline = Pipeline([
        ('smote', SMOTE(
            sampling_strategy=1.0,
            random_state=n_randstat
        )),
        ('rf', RandomForestClassifier(
            random_state=n_randstat,
            n_jobs=-1
        ))
    ])

    # Hyperparameter space for randomized search
    param_distributions = {
        'rf__n_estimators': randint(150, 600),
        'rf__max_depth': [None] + list(range(3, 31)),
        'rf__max_features': ['sqrt', 'log2', 0.5, 0.7, 1.0],
        'rf__min_samples_split': randint(2, 20),
        'rf__min_samples_leaf': randint(1, 10),
        'rf__bootstrap': [True, False],
    }

    # Scoring setup
    scoring = {
        'accuracy': make_scorer(accuracy_score, zero_division=0),
        'recall': make_scorer(recall_score, zero_division=0),
        'precision': make_scorer(precision_score, zero_division=0),
        'f1': make_scorer(f1_score, average='weighted', zero_division=0),
        'balanced_accuracy': make_scorer(balanced_accuracy_score),
    }

    # Nested CV setup
    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=n_randstat)
    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=n_randstat)

    outer_results = []
    outer_fold = 1

    for train_idx, test_idx in outer_cv.split(X, y):
        print(f"\n==============================")
        print(f"ðŸ”¹ Outer Fold {outer_fold} â€” training inner search for {anti}")
        print(f"==============================")

        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # ---- Inner hyperparameter tuning ----
        search = RandomizedSearchCV(
            estimator=base_pipeline,
            param_distributions=param_distributions,
            n_iter=30,                 # bump a bit vs 10 (tweak as you like)
            scoring='f1',              # weighted F1 via default binary? For binary labels, plain 'f1' is fine
            cv=inner_cv,
            random_state=n_randstat,
            n_jobs=-1,
            verbose=0
        )

        search.fit(X_train, y_train)

        print("Best inner parameters:", search.best_params_)
        print(f"Best inner score:", search.best_score_)

        # ---- Evaluate on outer fold ----
        best_model = search.best_estimator_
        y_pred = best_model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred, zero_division=0)
        precision = precision_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        fnr = 1 - recall

        outer_results.append({
            'antibiotic': anti,
            'fold': outer_fold,
            'best_params': search.best_params_,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy,
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'FNR': fnr
        })

        outer_fold += 1


    # Aggregate final performance for this antibiotic
    outer_df = pd.DataFrame(outer_results)
    print(f"\n\n===== Nested CV Summary for {anti} =====")
    print(outer_df[['fold', 'recall', 'f1', 'FNR']])
    print("Mean Accuracy:", outer_df['accuracy'].mean())
    print("Mean Recall:", outer_df['recall'].mean())
    print("Mean F1:", outer_df['f1'].mean())
    print("Mean false negative rate:", outer_df['FNR'].mean())

    # Save per-antibiotic result CSV
    output_path = f"results_{anti}_RF.csv"
    outer_df.to_csv(output_path, index=False)
    print(f"Saved results to {output_path}")

    # Add to global list
    all_results.append(outer_df)

# Merge all antibiotics' results together and save into .csv.
final_df = pd.concat(all_results, ignore_index=True)
final_df.to_csv("all_antibiotics_results_RF.csv", index=False)
print("Saved merged results to all_antibiotics_results_RF.csv")

import os

outdir = '/content/drive/MyDrive/DS_Project_2025/Modeling/Random_Forest/'
os.makedirs(outdir, exist_ok=True)

# Merge and save the combined results in the desired folder
final_df = pd.concat(all_results, ignore_index=True)
final_path = os.path.join(outdir, "all_antibiotics_results_RF.csv")
final_df.to_csv(final_path, index=False)
print(f"âœ… Saved merged results to {final_path}")

final_df